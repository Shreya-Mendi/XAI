{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "045e3cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image, ImageDraw, ImageOps, ImageFilter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e7fe0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "953 pineapple\n",
      "956 custard apple\n",
      "949 strawberry\n"
     ]
    }
   ],
   "source": [
    "# load ResNet34 pretrained on ImageNet1k (as instructed)\n",
    "model = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
    "model.eval().to(device)\n",
    "\n",
    "# load imagenet classes list (ensure imagenet_classes.txt is in notebook repo)\n",
    "# If you don't have it locally, add one to your repo. This cell will load it.\n",
    "classes_path = \"imagenet_classes.txt\"  # place or generate this file in colab\n",
    "with open(classes_path, \"r\") as f:\n",
    "    imagenet_classes = [line.strip() for line in f]\n",
    "\n",
    "# quick search utility:\n",
    "def find_class_substring(substr):\n",
    "    matches = [(i, c) for i, c in enumerate(imagenet_classes) if substr.lower() in c.lower()]\n",
    "    for idx, name in matches:\n",
    "        print(idx, name)\n",
    "    if not matches:\n",
    "        print(\"No matches for\", substr)\n",
    "\n",
    "# Example: show likely apple/strawberry matches (run to see actual names/indices)\n",
    "find_class_substring(\"apple\")\n",
    "find_class_substring(\"strawberry\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b94400a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a strawberry-shaped mask at a chosen patch size in pixels\n",
    "patch_size = 256  # patch resolution; adjust for printing DPI\n",
    "def make_strawberry_mask(size):\n",
    "    w, h = size, size\n",
    "    mask = Image.new(\"L\", (w,h), 0)\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    # simple heart/triangle+circle combo to approximate a strawberry shape\n",
    "    # lower big rounded part - ellipse\n",
    "    draw.ellipse([w*0.1, h*0.25, w*0.9, h*0.9], fill=255)\n",
    "    # top triangle/point to form a strawberry tip\n",
    "    draw.polygon([(w*0.5, h*0.05), (w*0.12, h*0.35), (w*0.88, h*0.35)], fill=255)\n",
    "    # optional small leaf area: cut out a hole for leaf at top (we'll overlay separate leaf later)\n",
    "    # draw.ellipse([w*0.36, h*0.0, w*0.64, h*0.26], fill=0)\n",
    "    mask = mask.filter(ImageFilter.GaussianBlur(radius=2))\n",
    "    return mask\n",
    "\n",
    "mask_pil = make_strawberry_mask(patch_size)\n",
    "mask_pil.save(\"strawberry_mask_preview.png\")\n",
    "mask_pil  # in Colab this will display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9237c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess for model input\n",
    "preprocess = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485,0.456,0.406],\n",
    "                std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "inv_normalize = T.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "\n",
    "def pil_to_tensor(img):\n",
    "    return T.ToTensor()(img).unsqueeze(0).to(device)\n",
    "\n",
    "def apply_patch_to_image(img_pil, patch_tensor, mask_pil, position=(0,0), scale=1.0, angle=0.0):\n",
    "    \"\"\"\n",
    "    img_pil: PIL RGB image\n",
    "    patch_tensor: CxHxW (0..1) tensor on CPU (or GPU — will convert to image)\n",
    "    mask_pil: PIL L mask at patch resolution\n",
    "    position: (x,y) top-left where the patch is applied on img (in pixels of target image)\n",
    "    scale: scale factor for patch size\n",
    "    angle: rotation in degrees\n",
    "    \"\"\"\n",
    "    img = img_pil.convert(\"RGBA\")\n",
    "    # convert patch tensor to PIL\n",
    "    patch_np = (patch_tensor.clamp(0,1).cpu().numpy() * 255).astype(np.uint8)\n",
    "    patch_pil = Image.fromarray(np.transpose(patch_np, (1,2,0)), mode=\"RGB\").convert(\"RGBA\")\n",
    "    mask_resized = mask_pil.resize(patch_pil.size, resample=Image.BILINEAR)\n",
    "    patch_pil.putalpha(mask_resized)\n",
    "\n",
    "    # scale and rotate patch\n",
    "    new_w = int(patch_pil.width * scale)\n",
    "    new_h = int(patch_pil.height * scale)\n",
    "    patch_pil = patch_pil.resize((new_w, new_h), Image.BILINEAR)\n",
    "    patch_pil = patch_pil.rotate(angle, expand=True, resample=Image.BILINEAR)\n",
    "    # reposition mask after rotation\n",
    "    # paste with alpha\n",
    "    x, y = position\n",
    "    tmp = Image.new(\"RGBA\", img.size)\n",
    "    tmp.paste(patch_pil, (x,y), patch_pil)\n",
    "    out = Image.alpha_composite(img, tmp).convert(\"RGB\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c02f7cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  242k  100  242k    0     0  1128k      0 --:--:-- --:--:-- --:--:-- 1130k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  562k  100  562k    0     0  1975k      0 --:--:-- --:--:-- --:--:-- 1981k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  273k  100  273k    0     0  1088k      0 --:--:-- --:--:-- --:--:-- 1092k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 99389  100 99389    0     0   490k      0 --:--:-- --:--:-- --:--:--  492k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2036k  100 2036k    0     0  6008k      0 --:--:-- --:--:-- --:--:-- 6024k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  392k  100  392k    0     0  1518k      0 --:--:-- --:--:-- --:--:-- 1520k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 22.4M  100 22.4M    0     0  20.3M      0  0:00:01  0:00:01 --:--:-- 20.3M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 23.1M  100 23.1M    0     0  21.4M      0  0:00:01  0:00:01 --:--:-- 21.4M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  651k  100  651k    0     0  1981k      0 --:--:-- --:--:-- --:--:-- 1984k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  793k  100  793k    0     0  2440k      0 --:--:-- --:--:-- --:--:-- 2433k\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p source_images\n",
    "!curl -L -o source_images/strawberry.jpg https://upload.wikimedia.org/wikipedia/commons/2/29/PerfectStrawberry.jpg\n",
    "!curl -L -o source_images/apple.jpg https://upload.wikimedia.org/wikipedia/commons/1/15/Red_Apple.jpg\n",
    "!curl -L -o source_images/cat.jpg https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg\n",
    "!curl -L -o source_images/dog.jpg https://upload.wikimedia.org/wikipedia/commons/9/9a/Pug_600.jpg\n",
    "!curl -L -o source_images/banana.jpg https://upload.wikimedia.org/wikipedia/commons/8/8a/Banana-Single.jpg\n",
    "!curl -L -o source_images/cup.jpg https://upload.wikimedia.org/wikipedia/commons/4/45/A_small_cup_of_coffee.JPG\n",
    "\n",
    "!curl -L -o source_images/strawberry2.jpg https://upload.wikimedia.org/wikipedia/commons/e/ef/Garden_strawberry_%28Fragaria_%C3%97_ananassa%29_halved.jpg\n",
    "!curl -L -o source_images/strawberry3.jpg https://upload.wikimedia.org/wikipedia/commons/4/4c/Garden_strawberry_%28Fragaria_%C3%97_ananassa%29_single2.jpg\n",
    "\n",
    "# Pineapples\n",
    "!curl -L -o source_images/pineapple1.jpg https://upload.wikimedia.org/wikipedia/commons/c/cb/Pineapple_and_cross_section.jpg\n",
    "!curl -L -o source_images/pineapple2.jpg https://upload.wikimedia.org/wikipedia/commons/3/3f/Pineapple-2540622.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38d9302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1500 [00:00<?, ?it/s]/var/folders/rn/5rl0r6jn4ws28cl_m7v_mf7h0000gn/T/ipykernel_70006/1516186588.py:29: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  patch_pil = Image.fromarray(np.transpose(patch_np, (1,2,0)), mode=\"RGB\").convert(\"RGBA\")\n",
      "  4%|▍         | 60/1500 [03:13<1:17:19,  3.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m patched_pil \u001b[38;5;241m=\u001b[39m apply_patch_to_image(img, patch\u001b[38;5;241m.\u001b[39mdetach(), mask_pil, position\u001b[38;5;241m=\u001b[39m(x,y), scale\u001b[38;5;241m=\u001b[39mscale, angle\u001b[38;5;241m=\u001b[39mangle)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# apply small random photometric changes for EOT\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m jitter \u001b[38;5;241m=\u001b[39m \u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mColorJitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbrightness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaturation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m patched_pil \u001b[38;5;241m=\u001b[39m jitter(patched_pil)\n\u001b[1;32m     45\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m preprocess(patched_pil)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/transforms/transforms.py:1191\u001b[0m, in \u001b[0;36mColorJitter.__init__\u001b[0;34m(self, brightness, contrast, saturation, hue)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mColorJitter\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Randomly change the brightness, contrast, saturation and hue of an image.\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;124;03m    If the image is torch Tensor, it is expected\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;124;03m    to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;124;03m            or use an interpolation that generates negative values before using this function.\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1191\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1192\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1193\u001b[0m         brightness: Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1194\u001b[0m         contrast: Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1195\u001b[0m         saturation: Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1196\u001b[0m         hue: Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1197\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m   1199\u001b[0m         _log_api_usage_once(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparams\n",
    "TARGET_CLASS = 953  # pineapple\n",
    "batch_size = 8\n",
    "num_steps = 1500\n",
    "lr = 0.1\n",
    "patch_res = patch_size\n",
    "patch_res = 256\n",
    "\n",
    "patch = torch.nn.Parameter(torch.rand(3, patch_res, patch_res, device=device) * 0.5 + 0.25)\n",
    "optimizer = torch.optim.Adam([patch], lr=0.1)\n",
    "\n",
    "# example dataset: use a small set of images to overlay on (you can load from your own directory)\n",
    "# Place some source images in folder \"source_images/\" (jpg/png). These should be innocuous photos (people, objects).\n",
    "from glob import glob\n",
    "source_paths = glob(\"source_images/*.jpg\") + glob(\"source_images/*.png\")\n",
    "assert len(source_paths) >= 8, \"Put at least a few images in source_images/ for training.\"\n",
    "\n",
    "def random_position_and_transform(img_w, img_h, patch_w, patch_h):\n",
    "    # choose random scale/position/angle that makes sense\n",
    "    scale = np.random.uniform(0.5, 1.2)\n",
    "    angle = np.random.uniform(-30,30)\n",
    "    # limit position so patch is fully on image\n",
    "    new_w = int(patch_w * scale)\n",
    "    new_h = int(patch_h * scale)\n",
    "    x = np.random.randint(0, max(1, img_w - new_w))\n",
    "    y = np.random.randint(0, max(1, img_h - new_h))\n",
    "    return (x,y), scale, angle\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for step in tqdm(range(num_steps)):\n",
    "    # sample a batch of source images\n",
    "    batch_paths = np.random.choice(source_paths, batch_size, replace=True)\n",
    "    images_tensor = []\n",
    "    for p in batch_paths:\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        iw, ih = img.size\n",
    "        (x,y), scale, angle = random_position_and_transform(iw, ih, patch_res, patch_res)\n",
    "        # apply the patch with small random color jitter\n",
    "        patched_pil = apply_patch_to_image(img, patch.detach(), mask_pil, position=(x,y), scale=scale, angle=angle)\n",
    "        # apply small random photometric changes for EOT\n",
    "        jitter = T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02)\n",
    "        patched_pil = jitter(patched_pil)\n",
    "        img_tensor = preprocess(patched_pil).to(device)\n",
    "        images_tensor.append(img_tensor)\n",
    "    images = torch.stack(images_tensor).to(device)\n",
    "    target = torch.full((images.size(0),), TARGET_CLASS, dtype=torch.long, device=device)\n",
    "\n",
    "    logits = model(images)\n",
    "    loss = loss_fn(logits, target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # gradient step on patch param\n",
    "    optimizer.step()\n",
    "\n",
    "    # clamp patch to [0,1]\n",
    "    with torch.no_grad():\n",
    "        patch.clamp_(0.0, 1.0)\n",
    "\n",
    "    # optional: decay lr or print stats\n",
    "    if (step+1) % 100 == 0:\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == target).float().mean().item()\n",
    "        print(f\"Step {step+1}/{num_steps}  loss={loss.item():.4f}  acc={acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c137a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save patch image (apply mask to reveal strawberry shape)\n",
    "final_patch_np = (patch.detach().cpu().numpy().transpose(1,2,0) * 255).astype(np.uint8)\n",
    "final_patch_pil = Image.fromarray(final_patch_np, \"RGB\")\n",
    "# apply mask to make transparent background where mask=0\n",
    "alpha = make_strawberry_mask(patch_res)\n",
    "final_rgba = final_patch_pil.convert(\"RGBA\")\n",
    "final_rgba.putalpha(alpha)\n",
    "final_rgba.save(\"final_strawberry_patch.png\")\n",
    "print(\"Saved final_strawberry_patch.png — download and print at 300 DPI for best results.\")\n",
    "final_rgba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = glob(\"test_images/*.jpg\") + glob(\"test_images/*.png\")\n",
    "for p in test_paths[:10]:\n",
    "    img = Image.open(p).convert(\"RGB\")\n",
    "    iw, ih = img.size\n",
    "    (x,y), scale, angle = random_position_and_transform(iw, ih, patch_res, patch_res)\n",
    "    patched_pil = apply_patch_to_image(img, patch.detach(), mask_pil, position=(x,y), scale=scale, angle=angle)\n",
    "    input_t = preprocess(patched_pil).unsqueeze(0).to(device)\n",
    "    logits = model(input_t)\n",
    "    pred = logits.argmax(dim=1).item()\n",
    "    print(\"File:\", p, \"=> predicted:\", imagenet_classes[pred])\n",
    "    display(patched_pil.resize((300,300)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
