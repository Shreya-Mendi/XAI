{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreya-Mendi/XAI/blob/Colab/Adversarial/patch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82c15d91",
      "metadata": {
        "id": "82c15d91"
      },
      "source": [
        "# Adversarial patch notebook — overview\n",
        "\n",
        "This notebook trains an adversarial patch that, when pasted onto images, causes a target model to predict the *pineapple* class (ImageNet class 953).\n",
        "\n",
        "A lot of the adversarial tasks code was influenced by the course github repo provided in class: https://github.com/AIPI-590-XAI/Duke-AI-XAI/blob/main/adversarial-ai-example-notebooks/adversarial_attacks_patches.ipynb\n",
        "\n",
        "High-level structure (map to code below):\n",
        "\n",
        "1. Install dependencies (Colab-ready): package installs for torch, torchvision and imaging libraries.\n",
        "2. Download dataset and pretrained resources: TinyImageNet archive and saved patches/checkpoints.\n",
        "3. Load model & dataset: load a pretrained ResNet-34, set to eval mode, and prepare a DataLoader for TinyImageNet images.\n",
        "4. Patch definition & helpers: create the strawberry mask, conversion helpers, and the differentiable placement function `place_patch_batch_tensor` which composites the patch into batches of images while allowing gradients to flow to the patch parameters.\n",
        "5. Training loop: optimize a small patch (tensor parameter) with a margin loss that encourages the patched images to be classified as the target class.\n",
        "6. Saving & evaluation: save the final patch image, evaluate targeted success rate on sample batches, and create upload/print-ready canvases.\n",
        "7. Visualization: show top-5 predictions for patched examples to inspect behavior.\n",
        "\n",
        "Notes and cautions:\n",
        "\n",
        "- Kept everything in a single cell as the cells were getting interupted during run-time and ending the training process and killing the kernel on vs code. Only worked continuously for me when kept in a single cell\n",
        "- The training loop can be compute-intensive; run on a GPU for reasonable runtimes.\n",
        "- The patch training strength depends on number of epochs, learning rate, and patch size — tune with care.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6655394a",
      "metadata": {
        "id": "6655394a",
        "outputId": "194b86f8-75bd-4ad1-9d10-4af4e31616c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426,
          "referenced_widgets": [
            "258c9b090f7942a89feb68cf9f804748",
            "0fbaaddf15c74c7fa58899e26e25e90f",
            "6f0e46dcd598433080c6e6677c4d2c7c",
            "7337c994921942a1b8d51292519fe7b9",
            "86143b4fab9a49f39a5e56bfd931cd20",
            "c3a4b6683f2b49d796fab7177a83cc4b",
            "105c956464604367959bd7e950aa05f5",
            "783382a426794b6dae2f408f08ba469b",
            "20d684c0cb8f4b94959cd5858fdc23ac",
            "299a238a38f141f9a402772d46e1d373",
            "85238927f17047058799768c0469ee6a"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial10/TinyImageNet.zip\n",
            "Unzipping ../data/TinyImageNet.zip\n",
            "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial10/patches.zip\n",
            "Unzipping ../saved_models/tutorial10/patches.zip\n",
            "Device: cpu\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 166MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset length: 5000\n",
            "Loaded 1000 labels\n",
            "Compiled model\n",
            "Training patch...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4009294350.py:232: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/8:   0%|          | 0/39 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "258c9b090f7942a89feb68cf9f804748"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4009294350.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "W1101 23:00:15.762000 393 torch/utils/cpp_extension.py:118] [0/0] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "/tmp/ipython-input-4009294350.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        }
      ],
      "source": [
        "# Strawberry -> Pineapple Adversarial Patch\n",
        "\n",
        "# This creates a new patch trained on the same TinyImageNet subset the course uses and saves\n",
        "# a strawberry-shaped patch that targets ImageNet class 953 (pineapple).\n",
        "\n",
        "\n",
        "# 1) Install dependencies\n",
        "\n",
        "# !pip install -q --upgrade pip\n",
        "# !pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "# !pip install -q pillow numpy tqdm matplotlib requests scikit-image opencv-python-headless\n",
        "\n",
        "\n",
        "# 2) Download TinyImageNet & pretrained resources (same as tutorial in the course github)\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from urllib.error import HTTPError\n",
        "import urllib.request\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "DATASET_PATH = \"../data\"\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial10\"\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial10/\"\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "files_to_get = [(DATASET_PATH, \"TinyImageNet.zip\"), (CHECKPOINT_PATH, \"patches.zip\")]\n",
        "for d, fname in files_to_get:\n",
        "    fpath = os.path.join(d, fname)\n",
        "    if not os.path.isfile(fpath):\n",
        "        url = base_url + fname\n",
        "        print(\"Downloading\", url)\n",
        "        try:\n",
        "            urllib.request.urlretrieve(url, fpath)\n",
        "        except HTTPError as e:\n",
        "            print(\"Download failed:\", e)\n",
        "        if fname.endswith('.zip') and os.path.isfile(fpath):\n",
        "            print(\"Unzipping\", fpath)\n",
        "            with zipfile.ZipFile(fpath, 'r') as z:\n",
        "                z.extractall(d)\n",
        "\n",
        "# 3) Load model, dataset, and helpers\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n",
        "\n",
        "# Load ResNet34 pretrained\n",
        "model = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "for p in model.parameters(): p.requires_grad = False\n",
        "\n",
        "# ImageNet normalization\n",
        "NORM_MEAN = [0.485, 0.456, 0.406]\n",
        "NORM_STD = [0.229, 0.224, 0.225]\n",
        "plain_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD)\n",
        "])\n",
        "\n",
        "# Load TinyImageNet dataset\n",
        "imagenet_path = os.path.join(DATASET_PATH, 'TinyImageNet')\n",
        "if not os.path.isdir(imagenet_path):\n",
        "    raise FileNotFoundError(f\"TinyImageNet not found at {imagenet_path}. Check downloads and rerun.\")\n",
        "\n",
        "dataset = ImageFolder(root=imagenet_path, transform=plain_transforms)\n",
        "print('Dataset length:', len(dataset))\n",
        "\n",
        "# DataLoader for training\n",
        "BATCH_SIZE = 128\n",
        "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, drop_last=True)\n",
        "\n",
        "# Load label names if available in TinyImageNet folder\n",
        "label_names_path = os.path.join(imagenet_path, 'label_list.json')\n",
        "if os.path.isfile(label_names_path):\n",
        "    with open(label_names_path, 'r') as f:\n",
        "        label_names = json.load(f)\n",
        "else:\n",
        "    # fallback to torchvision labels\n",
        "    labels_txt = 'imagenet_classes.txt'\n",
        "    if not os.path.isfile(labels_txt):\n",
        "        !wget -q -O imagenet_classes.txt https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
        "    with open('imagenet_classes.txt','r') as f:\n",
        "        label_names = [l.strip() for l in f.readlines()]\n",
        "print('Loaded', len(label_names), 'labels')\n",
        "\n",
        "\n",
        "# 4) Training : strawberry-shaped adversarial patch targeting pineapple (953)\n",
        "\n",
        "# Adjust PATCH_EPOCHS and BATCH_SIZE as needed if computationally heavy\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFilter\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# USER PARAMS\n",
        "TARGET_CLASS = 953   # pineapple\n",
        "PATCH_SIZE = 96      # 96x96 strong printable patch\n",
        "PATCH_EPOCHS = 8     # increase for stronger patch\n",
        "LR = 0.03\n",
        "CLIP_NORM = 0.1\n",
        "\n",
        "# strawberry mask\n",
        "def make_strawberry_mask(sz):\n",
        "    w,h = sz,sz\n",
        "    mask = Image.new('L', (w,h), 0)\n",
        "    d = ImageDraw.Draw(mask)\n",
        "    d.ellipse([w*0.12, h*0.28, w*0.88, h*0.92], fill=255)\n",
        "    d.polygon([(w*0.5,h*0.05),(w*0.12,h*0.35),(w*0.88,h*0.35)], fill=255)\n",
        "    return mask.filter(ImageFilter.GaussianBlur(radius=1))\n",
        "\n",
        "mask_pil = make_strawberry_mask(PATCH_SIZE)\n",
        "mask_alpha = transforms.ToTensor()(mask_pil).to(device)\n",
        "\n",
        "TENSOR_MEANS = torch.tensor(NORM_MEAN, device=device)[:,None,None]\n",
        "TENSOR_STD = torch.tensor(NORM_STD, device=device)[:,None,None]\n",
        "\n",
        "# mapping functions\n",
        "def patch_forward_norm(p):\n",
        "    return (torch.tanh(p) + 1.0 - 2.0 * TENSOR_MEANS) / (2.0 * TENSOR_STD)\n",
        "\n",
        "def patch_pixels(p):\n",
        "    return (torch.tanh(p).detach().cpu().numpy().transpose(1,2,0) + 1.0) / 2.0\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Differentiable place_patch_batch_tensor using affine transforms on GPU\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "def place_patch_batch_tensor(imgs_norm, patch_param, mask_alpha, min_scale=0.6, max_scale=1.0, max_angle=12):\n",
        "    \"\"\"\n",
        "    Differentiable placement of patch into a batch of normalized images.\n",
        "    Uses F.interpolate and F.affine_grid + F.grid_sample so gradients flow to patch_param.\n",
        "    imgs_norm: (N,3,H,W) normalized tensors on device\n",
        "    patch_param: (3,P,P) parameter on device (requires_grad=True)\n",
        "    mask_alpha: (1,P,P) mask tensor on device (values 0..1)\n",
        "    \"\"\"\n",
        "    device = imgs_norm.device\n",
        "    N, C, H, W = imgs_norm.shape\n",
        "    out = imgs_norm.clone()\n",
        "    P = patch_param.shape[1]\n",
        "\n",
        "    # normalized patch in model input space (differentiable)\n",
        "    patch_norm_full = patch_forward_norm(patch_param)  # (3,P,P) on device\n",
        "    alpha_full = mask_alpha  # (1,P,P) on device\n",
        "\n",
        "    for i in range(N):\n",
        "        scale = float(np.random.uniform(min_scale, max_scale))\n",
        "        angle = float(np.random.uniform(-max_angle, max_angle))\n",
        "        newP = max(1, int(P * scale))\n",
        "\n",
        "        # crop/resize the patch and alpha to newP via interpolate (diff)\n",
        "        patch_resized = F.interpolate(patch_norm_full.unsqueeze(0), size=(newP, newP),\n",
        "                                     mode='bilinear', align_corners=False).squeeze(0)  # (3,newP,newP)\n",
        "        alpha_resized = F.interpolate(alpha_full.unsqueeze(0), size=(newP, newP),\n",
        "                                      mode='bilinear', align_corners=False).squeeze(0)  # (1,newP,newP)\n",
        "\n",
        "        # If angle != 0, rotate using affine grid (differentiable)\n",
        "        if abs(angle) > 1e-3:\n",
        "            # Build affine transform matrix for rotation about center.\n",
        "            theta = math.radians(angle)\n",
        "            cos_t = math.cos(theta)\n",
        "            sin_t = math.sin(theta)\n",
        "            # rotation matrix (2x3) for grid_sample expects mapping from output -> input\n",
        "            # We want to rotate the patch by 'angle' degrees: use standard rotation matrix.\n",
        "            M = torch.tensor([[cos_t, -sin_t, 0.0],\n",
        "                              [sin_t,  cos_t, 0.0]], dtype=torch.float32, device=device)  # (2,3)\n",
        "\n",
        "            # grid_sample uses normalized coords; we want to rotate patch about its center.\n",
        "            # Create grid for the small patch size\n",
        "            grid = F.affine_grid(M.unsqueeze(0), size=(1, C, newP, newP), align_corners=False)  # (1,newP,newP,2)\n",
        "            # apply to patch and alpha\n",
        "            patch_resized = F.grid_sample(patch_resized.unsqueeze(0), grid, mode='bilinear', padding_mode='zeros', align_corners=False).squeeze(0)\n",
        "            alpha_resized = F.grid_sample(alpha_resized.unsqueeze(0), grid, mode='bilinear', padding_mode='zeros', align_corners=False).squeeze(0)\n",
        "\n",
        "        # choose position ensuring full in-bounds placement\n",
        "        max_x = max(0, W - newP)\n",
        "        max_y = max(0, H - newP)\n",
        "        x = np.random.randint(0, max_x+1) if max_x>0 else 0\n",
        "        y = np.random.randint(0, max_y+1) if max_y>0 else 0\n",
        "        x1, y1, x2, y2 = x, y, x + newP, y + newP\n",
        "\n",
        "        # region in out\n",
        "        out_region = out[i, :, y1:y2, x1:x2]\n",
        "        # sanity check sizes\n",
        "        if out_region.shape[1] != newP or out_region.shape[2] != newP:\n",
        "            continue\n",
        "\n",
        "        # composite: out = patch * alpha + out_region * (1-alpha)\n",
        "        alpha_resized = alpha_resized.clamp(0.0, 1.0)  # (1,newP,newP)\n",
        "        out[i, :, y1:y2, x1:x2] = patch_resized * alpha_resized + out_region * (1.0 - alpha_resized)\n",
        "\n",
        "    return out\n",
        "\n",
        "# optimizer & patch\n",
        "patch_param = nn.Parameter(torch.zeros(3, PATCH_SIZE, PATCH_SIZE, device=device))\n",
        "with torch.no_grad():\n",
        "    patch_param.normal_(mean=0.0, std=0.25)\n",
        "optimizer = torch.optim.Adam([patch_param], lr=LR)\n",
        "\n",
        "# margin loss function\n",
        "def margin_loss(logits, target_idx, margin=0.0):\n",
        "    tlog = logits[:, target_idx]\n",
        "    other = logits.clone()\n",
        "    other[:, target_idx] = -1e9\n",
        "    max_other, _ = other.max(dim=1)\n",
        "    return torch.clamp(max_other - tlog + margin, min=0.0).mean()\n",
        "\n",
        "# try torch.compile for speed\n",
        "try:\n",
        "    model = torch.compile(model)\n",
        "    print('Compiled model')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# training loop\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "print('Training patch...')\n",
        "for epoch in range(PATCH_EPOCHS):\n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{PATCH_EPOCHS}')\n",
        "    for imgs, labels in pbar:\n",
        "        imgs = imgs.to(device)\n",
        "        patched = place_patch_batch_tensor(imgs.clone(), patch_param, mask_alpha, min_scale=0.7, max_scale=1.0, max_angle=12)\n",
        "        target_labels = torch.full((patched.size(0),), TARGET_CLASS, dtype=torch.long, device=device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "            logits = model(patched)\n",
        "            loss = margin_loss(logits, TARGET_CLASS)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_([patch_param], CLIP_NORM)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        with torch.no_grad():\n",
        "            patch_param.clamp_(-3.0, 3.0)\n",
        "        with torch.no_grad():\n",
        "            preds = logits.argmax(dim=1)\n",
        "            batch_rate = (preds == TARGET_CLASS).float().mean().item()\n",
        "        pbar.set_postfix(loss=float(loss.item()), batch_rate=float(batch_rate))\n",
        "\n",
        "# save patch image\n",
        "final_pixels = patch_pixels(patch_param)\n",
        "pil_patch = Image.fromarray((final_pixels*255).astype(np.uint8))\n",
        "alpha = mask_pil.resize(pil_patch.size, Image.BILINEAR)\n",
        "pil_patch.putalpha(alpha)\n",
        "outname = f'strawberry_patch_pineapple_{PATCH_SIZE}px.png'\n",
        "pil_patch.save(outname)\n",
        "print('Saved patch to', outname)\n",
        "\n",
        "# quick eval function\n",
        "\n",
        "def eval_patch_on_loader(patch_param, loader, n_batches=50, trials_per_image=4):\n",
        "    total=0; successes=0\n",
        "    with torch.no_grad():\n",
        "        for i,(imgs,labels) in enumerate(loader):\n",
        "            imgs = imgs.to(device)\n",
        "            for _ in range(trials_per_image):\n",
        "                patched = place_patch_batch_tensor(imgs.clone(), patch_param, mask_alpha, min_scale=0.75, max_scale=1.0, max_angle=0)\n",
        "                logits = model(patched)\n",
        "                preds = logits.argmax(dim=1)\n",
        "                mask_non_target = (labels.to(device) != TARGET_CLASS)\n",
        "                successes += ((preds == TARGET_CLASS) & mask_non_target).sum().item()\n",
        "                total += mask_non_target.sum().item()\n",
        "            if i >= n_batches-1: break\n",
        "    return successes / max(1, total)\n",
        "\n",
        "print('Estimating success rate...')\n",
        "est = eval_patch_on_loader(patch_param, train_loader, n_batches=50, trials_per_image=4)\n",
        "print(f'Estimated targeted success: {est*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b0e9bac",
      "metadata": {
        "id": "0b0e9bac",
        "outputId": "7939e98c-01f4-4c22-b4e1-023ed617d8d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved upload-safe variants (no resizing of patch):\n",
            " - /Users/shreyamendi/XAI/Adversarial/patch_on_224_white.png\n",
            " - /Users/shreyamendi/XAI/Adversarial/patch_on_224_white_q95.jpg\n",
            " - /Users/shreyamendi/XAI/Adversarial/patch_on_224_black_q95.jpg\n",
            "Saved print-ready PNG: /Users/shreyamendi/XAI/Adversarial/patch_print_3in_300dpi.png\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# original saved patch\n",
        "orig_patch_path = outname  # 'strawberry_patch_pineapple_96px.png' from your script\n",
        "assert os.path.isfile(orig_patch_path), f\"Patch not found: {orig_patch_path}\"\n",
        "\n",
        "# load original patch\n",
        "orig = Image.open(orig_patch_path).convert(\"RGBA\")\n",
        "\n",
        "# create 224x224 canvas and paste original patch at native size (centered)\n",
        "CANVAS_SIZE = 224\n",
        "canvas_white = Image.new(\"RGBA\", (CANVAS_SIZE, CANVAS_SIZE), (255,255,255,255))\n",
        "x = (CANVAS_SIZE - orig.width) // 2\n",
        "y = (CANVAS_SIZE - orig.height) // 2\n",
        "canvas_white.paste(orig, (x,y), mask=orig)  # preserves the patch pixels, no scaling\n",
        "\n",
        "# convert to RGB (drops alpha) and save clean PNG and high-quality JPEG variants\n",
        "upload_png = f\"patch_on_224_white.png\"\n",
        "upload_jpg_q95 = f\"patch_on_224_white_q95.jpg\"\n",
        "\n",
        "canvas_white_rgb = canvas_white.convert(\"RGB\")\n",
        "canvas_white_rgb.save(upload_png, format=\"PNG\")\n",
        "canvas_white_rgb.save(upload_jpg_q95, format=\"JPEG\", quality=95)\n",
        "\n",
        "print(\"Saved upload-safe variants (no resizing of patch):\")\n",
        "print(\" -\", os.path.abspath(upload_png))\n",
        "print(\" -\", os.path.abspath(upload_jpg_q95))\n",
        "\n",
        "canvas_black = Image.new(\"RGBA\", (CANVAS_SIZE, CANVAS_SIZE), (0,0,0,255))\n",
        "canvas_black.paste(orig, (x,y), mask=orig)\n",
        "canvas_black.convert(\"RGB\").save(\"patch_on_224_black_q95.jpg\", format=\"JPEG\", quality=95)\n",
        "print(\" -\", os.path.abspath(\"patch_on_224_black_q95.jpg\"))\n",
        "\n",
        "\n",
        "# For printing,\n",
        "print_size_in = 3.0   # inches\n",
        "dpi = 300\n",
        "out_px = int(print_size_in * dpi)\n",
        "# create big canvas and paste a resized copy of the patch so print is crisp; change/remove if undesired\n",
        "big_canvas = Image.new(\"RGBA\", (out_px, out_px), (255,255,255,255))\n",
        "# scale factor to fill most of the big canvas but keep shape approx. If you want native-pixel-only, skip resize.\n",
        "scale = out_px / CANVAS_SIZE  # scales 224->out_px proportionally\n",
        "patched_resized = orig.resize((int(orig.width*scale), int(orig.height*scale)), Image.LANCZOS)\n",
        "bx = (out_px - patched_resized.width)//2\n",
        "by = (out_px - patched_resized.height)//2\n",
        "big_canvas.paste(patched_resized, (bx,by), mask=patched_resized)\n",
        "big_out = \"patch_print_3in_300dpi.png\"\n",
        "big_canvas.convert(\"RGB\").save(big_out, dpi=(dpi,dpi))\n",
        "print(\"Saved print-ready PNG:\", os.path.abspath(big_out))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68f61fdc",
      "metadata": {
        "id": "68f61fdc",
        "outputId": "5eb1a42b-f1fe-4bcc-85e9-f11a9ca7a01a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 0\n",
            "Top-5 indices: [953, 956, 363, 87, 954]\n",
            "Top-5 names: ['pineapple', 'custard apple', 'armadillo', 'African grey', 'banana']\n",
            "Top-5 probs: [9.9888402e-01 2.5211522e-04 2.0851233e-04 1.2242001e-04 9.8147786e-05]\n",
            "Example 1\n",
            "Top-5 indices: [953, 956, 131, 135, 954]\n",
            "Top-5 names: ['pineapple', 'custard apple', 'little blue heron', 'limpkin', 'banana']\n",
            "Top-5 probs: [9.9913824e-01 2.5549458e-04 1.8953078e-04 1.1735251e-04 8.2710525e-05]\n",
            "Example 2\n",
            "Top-5 indices: [953, 956, 411, 721, 529]\n",
            "Top-5 names: ['pineapple', 'custard apple', 'apron', 'pillow', 'diaper']\n",
            "Top-5 probs: [0.929177   0.02895793 0.01474138 0.00280569 0.00250522]\n",
            "Example 3\n",
            "Top-5 indices: [953, 954, 998, 987, 956]\n",
            "Top-5 names: ['pineapple', 'banana', 'ear', 'corn', 'custard apple']\n",
            "Top-5 probs: [9.9997890e-01 8.3316745e-06 3.1029329e-06 3.0556976e-06 2.6305511e-06]\n",
            "Example 4\n",
            "Top-5 indices: [953, 663, 698, 483, 956]\n",
            "Top-5 names: ['pineapple', 'monastery', 'palace', 'castle', 'custard apple']\n",
            "Top-5 probs: [0.993385   0.00163928 0.0014226  0.0010649  0.00103666]\n",
            "Example 5\n",
            "Top-5 indices: [953, 956, 954, 998, 987]\n",
            "Top-5 names: ['pineapple', 'custard apple', 'banana', 'ear', 'corn']\n",
            "Top-5 probs: [9.9991012e-01 2.9389872e-05 2.3395431e-05 1.9121035e-05 3.5254577e-06]\n"
          ]
        }
      ],
      "source": [
        "# show top-5 preds for some examples so we know what's happening\n",
        "import torch, numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "def show_top5_examples(patch_param, loader, n=5):\n",
        "    model.eval()\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs = imgs.to(device)[:8]\n",
        "            patched = place_patch_batch_tensor(imgs.clone(), patch_param, mask_alpha, min_scale=0.8, max_scale=1.0, max_angle=5)\n",
        "            logits = model(patched)\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            top5 = probs.topk(5, dim=1)\n",
        "            for i in range(patched.shape[0]):\n",
        "                print(\"Example\", cnt+i)\n",
        "                print(\"Top-5 indices:\", top5.indices[i].cpu().tolist())\n",
        "                print(\"Top-5 names:\", [label_names[idx] for idx in top5.indices[i].cpu().tolist()])\n",
        "                print(\"Top-5 probs:\", top5.values[i].cpu().numpy())\n",
        "                if i >= n-1: break\n",
        "            cnt += patched.shape[0]\n",
        "            if cnt >= n: break\n",
        "\n",
        "show_top5_examples(patch_param, train_loader, n=6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "383b5593",
      "metadata": {
        "id": "383b5593"
      },
      "source": [
        "\n",
        "\n",
        "**Acknowledgement / citation**\n",
        "\n",
        "Piece-wise notebook code and the training/image conversion tasks in this file were generated with the assistance of ChatGPT at 6:30pm on Nov 1.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "258c9b090f7942a89feb68cf9f804748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fbaaddf15c74c7fa58899e26e25e90f",
              "IPY_MODEL_6f0e46dcd598433080c6e6677c4d2c7c",
              "IPY_MODEL_7337c994921942a1b8d51292519fe7b9"
            ],
            "layout": "IPY_MODEL_86143b4fab9a49f39a5e56bfd931cd20"
          }
        },
        "0fbaaddf15c74c7fa58899e26e25e90f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3a4b6683f2b49d796fab7177a83cc4b",
            "placeholder": "​",
            "style": "IPY_MODEL_105c956464604367959bd7e950aa05f5",
            "value": "Epoch 1/8:  21%"
          }
        },
        "6f0e46dcd598433080c6e6677c4d2c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_783382a426794b6dae2f408f08ba469b",
            "max": 39,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20d684c0cb8f4b94959cd5858fdc23ac",
            "value": 8
          }
        },
        "7337c994921942a1b8d51292519fe7b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_299a238a38f141f9a402772d46e1d373",
            "placeholder": "​",
            "style": "IPY_MODEL_85238927f17047058799768c0469ee6a",
            "value": " 8/39 [07:26&lt;23:59, 46.45s/it, batch_rate=0, loss=11.8]"
          }
        },
        "86143b4fab9a49f39a5e56bfd931cd20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3a4b6683f2b49d796fab7177a83cc4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "105c956464604367959bd7e950aa05f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "783382a426794b6dae2f408f08ba469b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20d684c0cb8f4b94959cd5858fdc23ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "299a238a38f141f9a402772d46e1d373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85238927f17047058799768c0469ee6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}