{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7af7ffe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: timm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.0.20)\n",
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.10.5)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.7.1)\n",
      "Requirement already satisfied: grad-cam in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.5.5)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/shreyamendi/Library/Python/3.10/lib/python/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from timm) (0.34.4)\n",
      "Requirement already satisfied: safetensors in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from timm) (0.6.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shreyamendi/Library/Python/3.10/lib/python/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: ttach in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from grad-cam) (0.0.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from grad-cam) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface_hub->timm) (1.1.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "# Run in terminal or in a notebook cell prefixing ! if desired\n",
    "!pip install torch torchvision timm opencv-python matplotlib scikit-learn grad-cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb88c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, cv2, numpy as np, matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, ScoreCAM\n",
    "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, ScoreCAM\n",
    "from pytorch_grad_cam.utils.image import preprocess_image, show_cam_on_image\n",
    "\n",
    "\n",
    "IMG_SIZE = 224\n",
    "mean = [0.485, 0.456, 0.406]; std = [0.229, 0.224, 0.225]\n",
    "DATA_DIR = \"trafficnet_dataset_v1\"   # where zip extracted\n",
    "RESULT_DIR = \"results/cams\"\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55e65156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['accident', 'dense_traffic', 'fire', 'sparse_traffic']\n",
      "Train / Test sizes: 3600 800\n"
     ]
    }
   ],
   "source": [
    "# transforms\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "train_folder = os.path.join(DATA_DIR, \"train\")\n",
    "test_folder = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "train_ds = datasets.ImageFolder(train_folder, transform=train_tf)\n",
    "test_ds  = datasets.ImageFolder(test_folder, transform=eval_tf)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"Classes:\", train_ds.classes)\n",
    "print(\"Train / Test sizes:\", len(train_ds), len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40eee42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_ds.classes)\n",
    "model = timm.create_model('resnet50', pretrained=True, num_classes=num_classes)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "967d8529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 — train_acc: 0.686\n",
      "Epoch 2/3 — train_acc: 0.896\n",
      "Epoch 3/3 — train_acc: 0.933\n",
      "Saved model to models/resnet50_traffic.pth\n"
     ]
    }
   ],
   "source": [
    "do_train = True  # set False to skip training and use pretrained head (less accurate)\n",
    "if do_train:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    epochs = 3\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total, correct = 0, 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            preds = out.argmax(dim=1)\n",
    "            total += yb.size(0); correct += (preds==yb).sum().item()\n",
    "        train_acc = correct/total\n",
    "        print(f\"Epoch {epoch+1}/{epochs} — train_acc: {train_acc:.3f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    ckpt = \"models/resnet50_traffic.pth\"\n",
    "    os.makedirs(os.path.dirname(ckpt), exist_ok=True)\n",
    "    torch.save(model.state_dict(), ckpt)\n",
    "    print(\"Saved model to\", ckpt)\n",
    "else:\n",
    "    print(\"Skipping training; using model as-is (pretrained head).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f7f176e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      accident       0.92      0.89      0.91       200\n",
      " dense_traffic       0.93      0.93      0.93       200\n",
      "          fire       0.98      0.96      0.97       200\n",
      "sparse_traffic       0.85      0.91      0.87       200\n",
      "\n",
      "      accuracy                           0.92       800\n",
      "     macro avg       0.92      0.92      0.92       800\n",
      "  weighted avg       0.92      0.92      0.92       800\n",
      "\n",
      "Confusion matrix:\n",
      " [[178   3   1  18]\n",
      " [  3 185   0  12]\n",
      " [  5   0 192   3]\n",
      " [  7  10   2 181]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        out = model(xb)\n",
    "        preds = out.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=test_ds.classes))\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "491a5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rgb_image_resized(path, size=IMG_SIZE):\n",
    "    img_bgr = cv2.imread(path)\n",
    "    if img_bgr is None:\n",
    "        raise FileNotFoundError(path)\n",
    "    img_rgb = img_bgr[:, :, ::-1].astype(np.float32) / 255.0\n",
    "    img_rgb = cv2.resize(img_rgb, (size, size))\n",
    "    return img_rgb\n",
    "\n",
    "def prep_tensor(img_rgb):\n",
    "    # preprocess_image returns batched tensor [1,3,H,W]\n",
    "    return preprocess_image(img_rgb, mean=mean, std=std)  # returns torch tensor (cuda if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf3b9dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected samples:\n",
      "trafficnet_dataset_v1/test/accident/images_025 (2).jpg\n",
      "trafficnet_dataset_v1/test/dense_traffic/images_010.jpg\n",
      "trafficnet_dataset_v1/test/fire/images_007 (2).jpg\n",
      "trafficnet_dataset_v1/test/sparse_traffic/images_036 (3).jpg\n",
      "trafficnet_dataset_v1/test/dense_traffic/images_101.jpg\n"
     ]
    }
   ],
   "source": [
    "# sample one image per class from test set\n",
    "samples = []\n",
    "class_dirs = [os.path.join(test_folder, c) for c in test_ds.classes]\n",
    "for cdir in class_dirs:\n",
    "    files = [os.path.join(cdir, f) for f in os.listdir(cdir) if f.lower().endswith(('.jpg','.png','.jpeg'))]\n",
    "    if files:\n",
    "        samples.append(random.choice(files))\n",
    "\n",
    "# add one misclassified or hard example (optional): choose first misclassified from earlier predictions\n",
    "# Build map of test dataset index -> file path\n",
    "test_files = []\n",
    "for root, _, files in os.walk(test_folder):\n",
    "    for f in files:\n",
    "        if f.lower().endswith(('.jpg','.png','.jpeg')):\n",
    "            test_files.append(os.path.join(root, f))\n",
    "\n",
    "# If we have predictions from eval, choose a misclassified example\n",
    "# (This is a simple heuristic: find where predicted != true)\n",
    "if len(all_preds) == len(all_labels):\n",
    "    # map test_loader order to file paths is complex; simpler: sample extra random file to reach 5 examples\n",
    "    pass\n",
    "\n",
    "# ensure at least 5 images (duplicate if necessary)\n",
    "while len(samples) < 5:\n",
    "    samples.append(random.choice(test_files))\n",
    "\n",
    "print(\"Selected samples:\")\n",
    "for s in samples:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using target_layer: Bottleneck(\n",
      "  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop_block): Identity()\n",
      "  (act2): ReLU(inplace=True)\n",
      "  (aa): Identity()\n",
      "  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act3): ReLU(inplace=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 11/128 [00:15<02:46,  1.43s/it]"
     ]
    }
   ],
   "source": [
    "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, ScoreCAM, HiResCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# choose target layer (ResNet-style fallback)\n",
    "import torch.nn as nn\n",
    "if hasattr(model, 'layer4'):\n",
    "    target_layer = model.layer4[-1]\n",
    "else:\n",
    "    # fallback: pick last Conv2d in model\n",
    "    last_conv = None\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            last_conv = m\n",
    "    if last_conv is None:\n",
    "        raise RuntimeError(\"Could not find a Conv2d layer automatically; please pass the target layer manually.\")\n",
    "    target_layer = last_conv\n",
    "\n",
    "print(\"Using target_layer:\", target_layer)\n",
    "\n",
    "# Corrected run function using target_layers=[...]\n",
    "def run_cams_and_save_correct(img_path, model, target_layer, methods=['gradcam','gradcam++','scorecam'], save_dir=RESULT_DIR):\n",
    "    img_rgb = load_rgb_image_resized(img_path)         # HxWx3 float 0..1\n",
    "    input_tensor = prep_tensor(img_rgb).to(DEVICE)     # 1x3xHxW on DEVICE\n",
    "\n",
    "    cams = {}\n",
    "    # iterate methods and run each CAM in a try/except so one failing method doesn't stop the rest\n",
    "    for m in methods:\n",
    "        try:\n",
    "            if m == 'gradcam':\n",
    "                cam_obj = GradCAM(model=model, target_layers=[target_layer])\n",
    "            elif m == 'gradcam++':\n",
    "                cam_obj = GradCAMPlusPlus(model=model, target_layers=[target_layer])\n",
    "            elif m == 'scorecam':\n",
    "                cam_obj = ScoreCAM(model=model, target_layers=[target_layer])\n",
    "            elif m == 'hirescam':\n",
    "                cam_obj = HiResCAM(model=model, target_layers=[target_layer])\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Unknown cam: \" + str(m))\n",
    "\n",
    "            # execute cam (returns [batch, H, W])\n",
    "            grayscale = cam_obj(input_tensor=input_tensor, targets=None)[0]\n",
    "            cam_img = show_cam_on_image(img_rgb, grayscale, use_rgb=True)\n",
    "            cams[m] = cam_img\n",
    "\n",
    "        except Exception as err:\n",
    "            print(f\"{m} error on {os.path.basename(img_path)}: {err}\")\n",
    "        finally:\n",
    "            # try to delete cam object and free GPU memory to avoid destructor warnings/memory leaks\n",
    "            try:\n",
    "                del cam_obj\n",
    "            except Exception:\n",
    "                pass\n",
    "            if DEVICE.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Predict label + confidence for display\n",
    "    with torch.no_grad():\n",
    "        out = model(input_tensor)\n",
    "        probs = torch.softmax(out, dim=1).cpu().numpy()[0]\n",
    "        pred_idx = int(probs.argmax())\n",
    "        pred_conf = float(probs.max())\n",
    "        try:\n",
    "            pred_label = test_ds.classes[pred_idx]\n",
    "        except Exception:\n",
    "            pred_label = f\"idx={pred_idx}\"\n",
    "\n",
    "    # Plot original + cams side-by-side\n",
    "    n = 1 + len(cams)\n",
    "    plt.figure(figsize=(4*n, 4))\n",
    "    plt.subplot(1, n, 1)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(f\"Original\\nPred: {pred_label} ({pred_conf:.2f})\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    i = 2\n",
    "    for name, vis in cams.items():\n",
    "        plt.subplot(1, n, i)\n",
    "        plt.imshow(vis)\n",
    "        plt.title(name)\n",
    "        plt.axis('off')\n",
    "        i += 1\n",
    "    plt.show()\n",
    "\n",
    "    # Save images\n",
    "    base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    for name, vis in cams.items():\n",
    "        outpath = os.path.join(save_dir, f\"{base}_{name}.png\")\n",
    "        vis_u8 = (vis).astype(np.uint8)\n",
    "        # convert RGB->BGR for cv2.imwrite\n",
    "        cv2.imwrite(outpath, cv2.cvtColor(vis_u8, cv2.COLOR_RGB2BGR))\n",
    "    print(\"Saved cams for\", os.path.basename(img_path))\n",
    "    return pred_label, pred_conf\n",
    "\n",
    "# Run on samples (use your existing samples list)\n",
    "for s in samples:\n",
    "    try:\n",
    "        run_cams_and_save_correct(s, model, target_layer, methods=['gradcam','gradcam++','scorecam','hirescam'])\n",
    "    except Exception as e:\n",
    "        print(\"Error on\", s, \":\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
