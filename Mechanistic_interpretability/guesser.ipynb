{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dependencies ===\n",
    "import random, math, time, itertools\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "# set seed\n",
    "random.seed(0); np.random.seed(0); torch.manual_seed(0)\n",
    "\n",
    "# === Word list (small) ===\n",
    "# Replace with a larger word list if you like\n",
    "WORDS = [\"crane\",\"slate\",\"trace\",\"crate\",\"apple\",\"glare\",\"plane\",\"stare\",\"store\",\"stone\",\n",
    "         \"later\",\"hello\",\"world\",\"spear\",\"shiny\",\"piano\",\"other\",\"their\",\"there\",\"berry\",\n",
    "         \"cigar\",\"rebut\",\"sissy\",\"awake\",\"never\",\"adieu\",\"audio\",\"about\",\"trace\",\"later\"]\n",
    "WORDS = sorted(list(set(w for w in WORDS if len(w)==5)))\n",
    "V = len(WORDS)\n",
    "print(\"Vocab size:\", V)\n",
    "\n",
    "# maps\n",
    "LETTER2IDX = {c:i for i,c in enumerate(\"abcdefghijklmnopqrstuvwxyz\")}\n",
    "WORD2IDX = {w:i for i,w in enumerate(WORDS)}\n",
    "IDX2WORD = {i:w for w,i in WORD2IDX.items()}\n",
    "\n",
    "# --- wordle feedback function\n",
    "def wordle_feedback(target, guess):\n",
    "    fb = [0]*5\n",
    "    tlist = list(target)\n",
    "    # greens\n",
    "    for i in range(5):\n",
    "        if guess[i] == target[i]:\n",
    "            fb[i] = 2\n",
    "            tlist[i] = None\n",
    "    # yellows\n",
    "    for i in range(5):\n",
    "        if fb[i] == 0:\n",
    "            ch = guess[i]\n",
    "            if ch in tlist:\n",
    "                fb[i] = 1\n",
    "                tlist[tlist.index(ch)] = None\n",
    "    return fb\n",
    "\n",
    "# --- simulate games and build examples\n",
    "def generate_examples(N, K=5):\n",
    "    X_hist = []  # list of histories; each history is list of (guess, feedback) up to but not including next guess\n",
    "    Y_next = []  # next guess word index\n",
    "    for _ in range(N):\n",
    "        target = random.choice(WORDS)\n",
    "        # simple teacher: pick random consistent word each turn\n",
    "        history = []  # list of (guess_str, feedback_list)\n",
    "        candidates = set(WORDS)\n",
    "        for turn in range(K):\n",
    "            # sample a guess from current candidates (teacher)\n",
    "            guess = random.choice(list(candidates))\n",
    "            fb = wordle_feedback(target, guess)\n",
    "            X_hist.append(list(history))  # copy current history as training input\n",
    "            Y_next.append(WORD2IDX[guess])  # teacher's chosen guess\n",
    "            # update history and candidates\n",
    "            history.append((guess, fb))\n",
    "            # filter candidates by consistency with history\n",
    "            def consistent(word):\n",
    "                for g, f in history:\n",
    "                    if wordle_feedback(word, g) != f: return False\n",
    "                return True\n",
    "            candidates = set(w for w in candidates if consistent(w))\n",
    "            if len(candidates)==0:\n",
    "                candidates = set(WORDS)\n",
    "            if guess == target:\n",
    "                break\n",
    "    return X_hist, Y_next\n",
    "\n",
    "N = 30000\n",
    "K = 5\n",
    "X_hist, Y_next = generate_examples(N, K=K)\n",
    "print(\"Generated examples:\", len(X_hist))\n",
    "\n",
    "# === Dataset/encoding ===\n",
    "PAD_LET = \"<PAD>\"\n",
    "LET_V = 26\n",
    "def encode_history(history, K=5):\n",
    "    # returns tensor of shape (K, 5) with letter indices or PAD_LET coded as 26 for pad\n",
    "    # We encode guesses as sequences of letter indices; feedback as ints 0/1/2\n",
    "    pad_word = [26]*5\n",
    "    hist_enc = []\n",
    "    for turn in range(K):\n",
    "        if turn < len(history):\n",
    "            g, fb = history[turn]\n",
    "            letters = [LETTER2IDX[ch] for ch in g]\n",
    "            hist_enc.append((letters, fb))\n",
    "        else:\n",
    "            hist_enc.append((pad_word, [0]*5))\n",
    "    # flatten into a sequence: for each token we will create combined embedding (letter + feedback)\n",
    "    # we'll return two arrays: letters [K*5], feedbacks [K*5]\n",
    "    letters = []\n",
    "    fbs = []\n",
    "    for letters_t, fb_t in hist_enc:\n",
    "        letters.extend(letters_t)\n",
    "        fbs.extend(fb_t)\n",
    "    return np.array(letters, dtype=np.int64), np.array(fbs, dtype=np.int64)\n",
    "\n",
    "class GuesserDataset(Dataset):\n",
    "    def __init__(self, X_hist, Y_next, K=5):\n",
    "        self.X_hist = X_hist\n",
    "        self.Y_next = Y_next\n",
    "        self.K = K\n",
    "    def __len__(self): return len(self.X_hist)\n",
    "    def __getitem__(self, i):\n",
    "        letters, fbs = encode_history(self.X_hist[i], self.K)\n",
    "        return torch.tensor(letters), torch.tensor(fbs), torch.tensor(self.Y_next[i])\n",
    "\n",
    "ds = GuesserDataset(X_hist, Y_next, K=K)\n",
    "train_size = int(0.9*len(ds))\n",
    "val_size = len(ds)-train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size])\n",
    "train_dl = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=256)\n",
    "\n",
    "# === Model: Tiny Transformer ===\n",
    "class TinyGuesser(nn.Module):\n",
    "    def __init__(self, letter_vocab=27, fb_vocab=3, emb=32, nhead=2, nhid=64, nlayers=2, seq_len=K*5, vocab_out=V):\n",
    "        super().__init__()\n",
    "        self.letter_emb = nn.Embedding(letter_vocab, emb)\n",
    "        self.fb_emb = nn.Embedding(fb_vocab, 8)\n",
    "        self.token_proj = nn.Linear(emb+8, emb)  # combine letter+fb -> token embedding\n",
    "        self.pos_emb = nn.Embedding(seq_len, emb)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb, nhead=nhead, dim_feedforward=nhid)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n",
    "        self.pool = nn.Linear(emb, emb)  # simple pool to reduce to fixed vector (can do mean)\n",
    "        self.readout = nn.Sequential(\n",
    "            nn.Linear(emb, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, vocab_out)\n",
    "        )\n",
    "    def forward(self, letters, fbs):\n",
    "        # letters: [B, seq_len], fbs: [B, seq_len]\n",
    "        B, L = letters.shape\n",
    "        le = self.letter_emb(letters)    # [B,L,emb]\n",
    "        fe = self.fb_emb(fbs)           # [B,L,8]\n",
    "        tok = torch.cat([le, fe], dim=-1)\n",
    "        tok = self.token_proj(tok)      # [B,L,emb]\n",
    "        pos = torch.arange(L, device=letters.device).unsqueeze(0).repeat(B,1)\n",
    "        tok = tok + self.pos_emb(pos)\n",
    "        # transformer expects [L,B,emb]\n",
    "        h = self.transformer(tok.permute(1,0,2))  # [L,B,emb]\n",
    "        h = h.permute(1,0,2)  # [B,L,emb]\n",
    "        # pool by mean\n",
    "        v = h.mean(dim=1)\n",
    "        logits = self.readout(v)  # [B, V]\n",
    "        return logits, h  # return token-level hidden for probes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TinyGuesser().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# === Training ===\n",
    "def evaluate(dl):\n",
    "    model.eval()\n",
    "    total=0; correct=0; loss_sum=0.0\n",
    "    with torch.no_grad():\n",
    "        for letters, fbs, y in dl:\n",
    "            letters=fletters=letters.to(device); fbs=fbs.to(device); y=y.to(device)\n",
    "            logits, _ = model(letters, fbs)\n",
    "            loss = loss_fn(logits, y)\n",
    "            loss_sum += loss.item()*letters.size(0)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds==y).sum().item()\n",
    "            total += letters.size(0)\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "# train loop\n",
    "for epoch in range(1,21):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    for letters, fbs, y in train_dl:\n",
    "        letters=letters.to(device); fbs=fbs.to(device); y=y.to(device)\n",
    "        logits, _ = model(letters, fbs)\n",
    "        loss = loss_fn(logits, y)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    val_loss, val_acc = evaluate(val_dl)\n",
    "    print(f\"Epoch {epoch}  val_loss {val_loss:.4f}  val_acc {val_acc:.4f}  time {time.time()-t0:.1f}s\")\n",
    "    if val_acc > 0.7: break  # early stop for small vocabs\n",
    "\n",
    "# save model if wished\n",
    "torch.save(model.state_dict(), \"tiny_guesser.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
