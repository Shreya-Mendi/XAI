{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreya-Mendi/XAI/blob/Colab/Mechanistic_interpretability/guesser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e842ac9c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e842ac9c",
        "outputId": "341a441d-2c75-4d55-8d02-b7963914aee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 10657\n"
          ]
        }
      ],
      "source": [
        "# === Dependencies ===\n",
        "import random, math, time, itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "# set seed\n",
        "random.seed(0); np.random.seed(0); torch.manual_seed(0)\n",
        "\n",
        "# === Word list (small) ===\n",
        "# Replace with a larger word list if you like\n",
        "WORDS = pd.read_csv(\"/content/valid_guesses.csv\", header=None)[0].tolist()\n",
        "WORDS = sorted(list(set(w for w in WORDS if len(w)==5)))\n",
        "V = len(WORDS)\n",
        "print(\"Vocab size:\", V)\n",
        "\n",
        "# maps\n",
        "LETTER2IDX = {c:i for i,c in enumerate(\"abcdefghijklmnopqrstuvwxyz\")}\n",
        "WORD2IDX = {w:i for i,w in enumerate(WORDS)}\n",
        "IDX2WORD = {i:w for w,i in WORD2IDX.items()}\n",
        "\n",
        "# --- wordle feedback function\n",
        "def wordle_feedback(target, guess):\n",
        "    fb = [0]*5\n",
        "    tlist = list(target)\n",
        "    # greens\n",
        "    for i in range(5):\n",
        "        if guess[i] == target[i]:\n",
        "            fb[i] = 2\n",
        "            tlist[i] = None\n",
        "    # yellows\n",
        "    for i in range(5):\n",
        "        if fb[i] == 0:\n",
        "            ch = guess[i]\n",
        "            if ch in tlist:\n",
        "                fb[i] = 1\n",
        "                tlist[tlist.index(ch)] = None\n",
        "    return fb\n",
        "\n",
        "# --- simulate games and build examples\n",
        "def generate_examples(N, K=5):\n",
        "    X_hist = []  # list of histories; each history is list of (guess, feedback) up to but not including next guess\n",
        "    Y_next = []  # next guess word index\n",
        "    for _ in range(N):\n",
        "        target = random.choice(WORDS)\n",
        "        # simple teacher: pick random consistent word each turn\n",
        "        history = []  # list of (guess_str, feedback_list)\n",
        "        candidates = set(WORDS)\n",
        "        for turn in range(K):\n",
        "            # sample a guess from current candidates (teacher)\n",
        "            guess = random.choice(list(candidates))\n",
        "            fb = wordle_feedback(target, guess)\n",
        "            X_hist.append(list(history))  # copy current history as training input\n",
        "            Y_next.append(WORD2IDX[guess])  # teacher's chosen guess\n",
        "            # update history and candidates\n",
        "            history.append((guess, fb))\n",
        "            # filter candidates by consistency with history\n",
        "            def consistent(word):\n",
        "                for g, f in history:\n",
        "                    if wordle_feedback(word, g) != f: return False\n",
        "                return True\n",
        "            candidates = set(w for w in candidates if consistent(w))\n",
        "            if len(candidates)==0:\n",
        "                candidates = set(WORDS)\n",
        "            if guess == target:\n",
        "                break\n",
        "    return X_hist, Y_next\n",
        "\n",
        "N = 30000\n",
        "K = 5\n",
        "X_hist, Y_next = generate_examples(N, K=K)\n",
        "print(\"Generated examples:\", len(X_hist))\n",
        "\n",
        "# === Dataset/encoding ===\n",
        "PAD_LET = \"<PAD>\"\n",
        "LET_V = 26\n",
        "def encode_history(history, K=5):\n",
        "    # returns tensor of shape (K, 5) with letter indices or PAD_LET coded as 26 for pad\n",
        "    # We encode guesses as sequences of letter indices; feedback as ints 0/1/2\n",
        "    pad_word = [26]*5\n",
        "    hist_enc = []\n",
        "    for turn in range(K):\n",
        "        if turn < len(history):\n",
        "            g, fb = history[turn]\n",
        "            letters = [LETTER2IDX[ch] for ch in g]\n",
        "            hist_enc.append((letters, fb))\n",
        "        else:\n",
        "            hist_enc.append((pad_word, [0]*5))\n",
        "    # flatten into a sequence: for each token we will create combined embedding (letter + feedback)\n",
        "    # we'll return two arrays: letters [K*5], feedbacks [K*5]\n",
        "    letters = []\n",
        "    fbs = []\n",
        "    for letters_t, fb_t in hist_enc:\n",
        "        letters.extend(letters_t)\n",
        "        fbs.extend(fb_t)\n",
        "    return np.array(letters, dtype=np.int64), np.array(fbs, dtype=np.int64)\n",
        "\n",
        "class GuesserDataset(Dataset):\n",
        "    def __init__(self, X_hist, Y_next, K=5):\n",
        "        self.X_hist = X_hist\n",
        "        self.Y_next = Y_next\n",
        "        self.K = K\n",
        "    def __len__(self): return len(self.X_hist)\n",
        "    def __getitem__(self, i):\n",
        "        letters, fbs = encode_history(self.X_hist[i], self.K)\n",
        "        return torch.tensor(letters), torch.tensor(fbs), torch.tensor(self.Y_next[i])\n",
        "\n",
        "ds = GuesserDataset(X_hist, Y_next, K=K)\n",
        "train_size = int(0.9*len(ds))\n",
        "val_size = len(ds)-train_size\n",
        "train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size])\n",
        "train_dl = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=256)\n",
        "\n",
        "# === Model: Tiny Transformer ===\n",
        "class TinyGuesser(nn.Module):\n",
        "    def __init__(self, letter_vocab=27, fb_vocab=3, emb=32, nhead=2, nhid=64, nlayers=2, seq_len=K*5, vocab_out=V):\n",
        "        super().__init__()\n",
        "        self.letter_emb = nn.Embedding(letter_vocab, emb)\n",
        "        self.fb_emb = nn.Embedding(fb_vocab, 8)\n",
        "        self.token_proj = nn.Linear(emb+8, emb)  # combine letter+fb -> token embedding\n",
        "        self.pos_emb = nn.Embedding(seq_len, emb)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb, nhead=nhead, dim_feedforward=nhid)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n",
        "        self.pool = nn.Linear(emb, emb)  # simple pool to reduce to fixed vector (can do mean)\n",
        "        self.readout = nn.Sequential(\n",
        "            nn.Linear(emb, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, vocab_out)\n",
        "        )\n",
        "    def forward(self, letters, fbs):\n",
        "        # letters: [B, seq_len], fbs: [B, seq_len]\n",
        "        B, L = letters.shape\n",
        "        le = self.letter_emb(letters)    # [B,L,emb]\n",
        "        fe = self.fb_emb(fbs)           # [B,L,8]\n",
        "        tok = torch.cat([le, fe], dim=-1)\n",
        "        tok = self.token_proj(tok)      # [B,L,emb]\n",
        "        pos = torch.arange(L, device=letters.device).unsqueeze(0).repeat(B,1)\n",
        "        tok = tok + self.pos_emb(pos)\n",
        "        # transformer expects [L,B,emb]\n",
        "        h = self.transformer(tok.permute(1,0,2))  # [L,B,emb]\n",
        "        h = h.permute(1,0,2)  # [B,L,emb]\n",
        "        # pool by mean\n",
        "        v = h.mean(dim=1)\n",
        "        logits = self.readout(v)  # [B, V]\n",
        "        return logits, h  # return token-level hidden for probes\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TinyGuesser().to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# === Training ===\n",
        "# inside train loop: collect train_loss/val_loss/val_acc/top5\n",
        "# compute top5:\n",
        "def topk_acc(logits, y, k=5):\n",
        "    topk = logits.topk(k, dim=-1).indices\n",
        "    return (topk == y.unsqueeze(-1)).any(dim=-1).float().mean().item()\n",
        "\n",
        "def evaluate(dl):\n",
        "    model.eval()\n",
        "    total=0; correct=0; loss_sum=0.0\n",
        "    with torch.no_grad():\n",
        "        for letters, fbs, y in dl:\n",
        "            letters=fletters=letters.to(device); fbs=fbs.to(device); y=y.to(device)\n",
        "            logits, _ = model(letters, fbs)\n",
        "            loss = loss_fn(logits, y)\n",
        "            loss_sum += loss.item()*letters.size(0)\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            correct += (preds==y).sum().item()\n",
        "            total += letters.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "# train loop\n",
        "for epoch in range(1,21):\n",
        "    model.train()\n",
        "    t0 = time.time()\n",
        "    for letters, fbs, y in train_dl:\n",
        "        letters=letters.to(device); fbs=fbs.to(device); y=y.to(device)\n",
        "        logits, _ = model(letters, fbs)\n",
        "        loss = loss_fn(logits, y)\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "    val_loss, val_acc = evaluate(val_dl)\n",
        "    print(f\"Epoch {epoch}  val_loss {val_loss:.4f}  val_acc {val_acc:.4f}  time {time.time()-t0:.1f}s\")\n",
        "    if val_acc > 0.7: break  # early stop for small vocabs\n",
        "\n",
        "# save model if wished\n",
        "torch.save(model.state_dict(), \"tiny_guesser.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show which vocab logits are most affected by pooled neuron i\n",
        "neuron_i = most_harmful[0]  # from your ablation result\n",
        "W_vocab = model.readout[-1].weight.detach().cpu().numpy()  # shape (V, 64)\n",
        "# if readout is Sequential(Lin(emb,64),ReLU,Lin(64,V)), adjust this to inspect last linear\n",
        "# We'll inspect the second-to-last layer mapping for interpretability:\n",
        "row = model.readout[-1].weight.detach().cpu().numpy()[:, neuron_i]  # contribution of v[:,i] across vocab\n",
        "ranked = np.argsort(row)[::-1]\n",
        "for idx in ranked[:8]:\n",
        "    print(IDX2WORD[idx], row[idx])\n"
      ],
      "metadata": {
        "id": "fAljgMDlRPLI"
      },
      "id": "fAljgMDlRPLI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(range(len(row)), row)  # x = indices, height = row values\n",
        "plt.title(\"Readout weights for word 'crane'\")\n",
        "plt.xlabel(\"Hidden feature index\")\n",
        "plt.ylabel(\"Weight value\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "sorted_idx = np.argsort(row)\n",
        "plt.bar(range(len(row)), row[sorted_idx])\n",
        "plt.title(\"Sorted readout weights for 'crane'\")\n",
        "plt.xlabel(\"Sorted feature index\")\n",
        "plt.ylabel(\"Weight value\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Nou1r8XaLJrm"
      },
      "id": "Nou1r8XaLJrm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick many examples where the teacher must choose between words that differ in a key letter.\n",
        "# compute token-level activations h (B,L,emb) and visualize mean activations across tokens or neurons.\n",
        "letters, fbs, y = next(iter(val_dl))\n",
        "letters, fbs = letters.to(device), fbs.to(device)\n",
        "logits, h = model(letters, fbs)  # h: [B, L, emb]\n",
        "act = h.mean(dim=1).cpu().detach().numpy()  # [B, emb]\n",
        "plt.imshow(act.T, aspect='auto'); plt.colorbar(); plt.title(\"Neuron activations (examples x neurons)\"); plt.show()\n"
      ],
      "metadata": {
        "id": "MrrGiJraLKqo"
      },
      "id": "MrrGiJraLKqo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Saliency via torch.autograd.grad (robust) ---\n",
        "model.eval()\n",
        "\n",
        "# grab one example\n",
        "letters, fbs, y = next(iter(val_dl))\n",
        "letters, fbs = letters.to(device), fbs.to(device)\n",
        "letters = letters[0:1]   # (1, seq_len)\n",
        "fbs = fbs[0:1]\n",
        "\n",
        "# get embeddings (float tensors)\n",
        "letters_emb = model.letter_emb(letters)   # (1, seq_len, emb)\n",
        "fbs_emb = model.fb_emb(fbs)               # (1, seq_len, 8)\n",
        "tok = torch.cat([letters_emb, fbs_emb], dim=-1)    # (1, seq_len, emb+8)\n",
        "tok = model.token_proj(tok)              # (1, seq_len, emb)\n",
        "# ensure torch knows we want grads w.r.t. tok\n",
        "tok.requires_grad_(True)\n",
        "\n",
        "# forward through the rest of the model using tok as input to transformer:\n",
        "pos = torch.arange(tok.size(1), device=device).unsqueeze(0)\n",
        "tok_pos = tok + model.pos_emb(pos)       # (1, seq_len, emb)\n",
        "h = model.transformer(tok_pos.permute(1,0,2))  # [L,B,emb]\n",
        "h = h.permute(1,0,2)                     # [B,L,emb]\n",
        "v = h.mean(dim=1)                        # [B, emb]\n",
        "logits = model.readout(v)                # [B, V]\n",
        "\n",
        "# choose word to inspect\n",
        "target_word = \"crane\"\n",
        "score = logits[0, WORD2IDX[target_word]]\n",
        "\n",
        "# compute gradients of score w.r.t tok\n",
        "grads = torch.autograd.grad(score, tok)[0]   # shape: (1, seq_len, emb)\n",
        "grads = grads.detach().cpu().numpy()[0]      # (seq_len, emb)\n",
        "\n",
        "# aggregate per-token saliency (L2 norm over embedding dim)\n",
        "saliency_per_token = np.linalg.norm(grads, axis=1)  # (seq_len,)\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize=(12,3))\n",
        "plt.bar(np.arange(len(saliency_per_token)), saliency_per_token)\n",
        "plt.xlabel(\"Token index (turn Ã— position)\")\n",
        "plt.ylabel(\"Saliency (L2 of embedding gradient)\")\n",
        "plt.title(f\"Token saliency for predicting '{target_word}'\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "E9eN4wdeLPDE"
      },
      "id": "E9eN4wdeLPDE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Utilities to compute pooled vector and do readout separately ----\n",
        "device = next(model.parameters()).device\n",
        "\n",
        "def compute_pooled_vector(model, letters, fbs):\n",
        "    \"\"\"\n",
        "    Run the model up to the pooling step and return:\n",
        "      v: pooled vector of shape [B, emb]\n",
        "      h: token-level hidden [B, L, emb] (optional, returned for probes)\n",
        "    \"\"\"\n",
        "    # letters: [B, seq_len] (long), fbs: [B, seq_len] (long)\n",
        "    with torch.no_grad():\n",
        "        le = model.letter_emb(letters)    # [B,L,emb]\n",
        "        fe = model.fb_emb(fbs)            # [B,L,8]\n",
        "        tok = torch.cat([le, fe], dim=-1) # [B,L,emb+8]\n",
        "        tok = model.token_proj(tok)       # [B,L,emb]\n",
        "        pos = torch.arange(tok.size(1), device=letters.device).unsqueeze(0).repeat(letters.size(0),1)\n",
        "        tok = tok + model.pos_emb(pos)    # add pos emb\n",
        "        h = model.transformer(tok.permute(1,0,2))  # [L,B,emb]\n",
        "        h = h.permute(1,0,2)              # [B,L,emb]\n",
        "        v = h.mean(dim=1)                 # [B, emb]\n",
        "    return v, h\n",
        "\n",
        "def readout_from_pooled(model, v):\n",
        "    \"\"\"\n",
        "    Apply the model.readout to pooled vector v (no grad) -> logits [B, V]\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        logits = model.readout(v)  # [B, V]\n",
        "    return logits\n",
        "\n",
        "# ---- Single-example neuron ablation test (like your starter single-sample ablation) ----\n",
        "# pick one sample from val_dl\n",
        "letters_batch, fbs_batch, y_batch = next(iter(val_dl))\n",
        "letters_batch = letters_batch.to(device); fbs_batch = fbs_batch.to(device); y_batch = y_batch.to(device)\n",
        "\n",
        "# choose the first example in the batch to inspect\n",
        "letters_ex = letters_batch[0:1]; fbs_ex = fbs_batch[0:1]; y_ex = y_batch[0:1]\n",
        "\n",
        "# baseline prediction\n",
        "v_orig, _ = compute_pooled_vector(model, letters_ex, fbs_ex)\n",
        "logits_orig = readout_from_pooled(model, v_orig)\n",
        "pred_orig = logits_orig.argmax(dim=-1).item()\n",
        "print(\"Baseline pred:\", IDX2WORD[pred_orig], \"true:\", IDX2WORD[y_ex.item()])\n",
        "\n",
        "# ablate neuron i\n",
        "neuron_to_zero = 3\n",
        "v_ablate = v_orig.clone()\n",
        "v_ablate[:, neuron_to_zero] = 0.0\n",
        "logits_ablate = readout_from_pooled(model, v_ablate)\n",
        "pred_ablate = logits_ablate.argmax(dim=-1).item()\n",
        "print(f\"After zeroing neuron {neuron_to_zero}: pred:\", IDX2WORD[pred_ablate])\n",
        "\n",
        "# show change in top-5 logits if useful\n",
        "import numpy as np\n",
        "probs_orig = torch.softmax(logits_orig, dim=-1).cpu().numpy()[0]\n",
        "probs_ab = torch.softmax(logits_ablate, dim=-1).cpu().numpy()[0]\n",
        "topk = 8\n",
        "print(\"Top probs before (word:prob)  |  after\")\n",
        "for idx in np.argsort(probs_orig)[-topk:][::-1]:\n",
        "    print(f\"{IDX2WORD[idx]}:{probs_orig[idx]:.3f}  |  {probs_ab[idx]:.3f}\")\n",
        "\n",
        "\n",
        "def examples_with_green_at_position(dataset, pos):\n",
        "    # dataset is the underlying ds or train_ds/val_ds subset; assume each example __getitem__ returns letters,fbs,y\n",
        "    idxs = []\n",
        "    for i in range(len(dataset)):\n",
        "        letters, fbs, y = dataset[i]\n",
        "        fbs = np.array(fbs)\n",
        "        # fbs flattened: K*5 elements\n",
        "        if (fbs.reshape(K,5)[:, pos] == 2).any():\n",
        "            idxs.append(i)\n",
        "    return idxs\n",
        "\n",
        "# make a DataLoader for that subset:\n",
        "green_idxs = examples_with_green_at_position(val_ds.dataset if hasattr(val_ds, 'dataset') else val_ds, pos=2)\n",
        "from torch.utils.data import Subset\n",
        "green_loader = DataLoader(Subset(val_ds.dataset, green_idxs), batch_size=128)\n",
        "# then call eval_with_neuron_ablation(model, neuron_idx, green_loader)\n",
        "\n",
        "\n",
        "# ---- Sweep ablation across all pooled neurons (like your starter sweep) ----\n",
        "def eval_with_neuron_ablation(model, neuron_idx, dataloader):\n",
        "    \"\"\"\n",
        "    Evaluate accuracy on dataloader when pooled neuron neuron_idx is zeroed.\n",
        "    Returns accuracy (top-1) across full dataloader.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for letters, fbs, y in dataloader:\n",
        "            letters = letters.to(device); fbs = fbs.to(device); y = y.to(device)\n",
        "            v, _ = compute_pooled_vector(model, letters, fbs)   # [B, emb]\n",
        "            v[:, neuron_idx] = 0.0\n",
        "            logits = readout_from_pooled(model, v)             # [B, V]\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# Baseline accuracy (no ablation)\n",
        "def evaluate_baseline(model, dataloader):\n",
        "    model.eval()\n",
        "    total=0; correct=0\n",
        "    with torch.no_grad():\n",
        "        for letters, fbs, y in dataloader:\n",
        "            letters = letters.to(device); fbs = fbs.to(device); y = y.to(device)\n",
        "            logits, _ = model(letters, fbs)\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct/total\n",
        "\n",
        "baseline_acc = evaluate_baseline(model, val_dl)\n",
        "print(\"Baseline val accuracy:\", baseline_acc)\n",
        "\n",
        "# Sweep all pooled dimensions\n",
        "emb_dim = v_orig.size(1)  # pooled embedding dimension\n",
        "neuron_indices = list(range(emb_dim))\n",
        "signed_deltas = []\n",
        "for i in neuron_indices:\n",
        "    acc_i = eval_with_neuron_ablation(model, i, val_dl)\n",
        "    signed_deltas.append(acc_i - baseline_acc)  # drop is negative if accuracy decreases\n",
        "\n",
        "# Plot signed changes (delta accuracy)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12,4))\n",
        "colors = ['red' if d < 0 else 'green' if d > 0 else 'gray' for d in signed_deltas]\n",
        "plt.bar(neuron_indices, signed_deltas, color=colors)\n",
        "plt.axhline(0, color='black', linestyle='--')\n",
        "plt.xlabel(\"Pooled neuron index\")\n",
        "plt.ylabel(\"Accuracy change (ablate - baseline)\")\n",
        "plt.title(\"Effect of zeroing pooled neurons on validation accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# Print the most impactful neurons (largest negative delta)\n",
        "deltas = np.array(signed_deltas)\n",
        "most_harmful = np.argsort(deltas)[:8]   # most negative (harmful ablations)\n",
        "print(\"Most harmful ablations (neuron -> delta):\")\n",
        "for i in most_harmful:\n",
        "    print(i, deltas[i])\n"
      ],
      "metadata": {
        "id": "INEzicFCLPk_"
      },
      "id": "INEzicFCLPk_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_history(letters_tensor, fbs_tensor, K=K):\n",
        "    # letters_tensor: tensor shape [seq_len], fbs_tensor same\n",
        "    seq_len = K * 5\n",
        "    letters = letters_tensor.cpu().numpy().tolist()\n",
        "    fbs = fbs_tensor.cpu().numpy().tolist()\n",
        "    lines = []\n",
        "    for t in range(K):\n",
        "        chunk_letters = letters[t*5:(t+1)*5]\n",
        "        chunk_fbs = fbs[t*5:(t+1)*5]\n",
        "        # skip fully padded turns\n",
        "        if all(l == 26 for l in chunk_letters):\n",
        "            continue\n",
        "        word = ''.join([chr(ord('a') + l) if l<26 else '_' for l in chunk_letters])\n",
        "        # pretty feedback e.g., G/Y/-\n",
        "        fbstr = ''.join(['G' if c==2 else 'Y' if c==1 else '-' for c in chunk_fbs])\n",
        "        lines.append(f\"Turn{t}: {word}   [{fbstr}]\")\n",
        "    return \"\\n\".join(lines)\n"
      ],
      "metadata": {
        "id": "YWrufYhPQxug"
      },
      "id": "YWrufYhPQxug",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    letters, fbs, y = next(iter(val_dl))\n",
        "    letters, fbs, y = letters.to(device), fbs.to(device), y.to(device)\n",
        "    logits, _ = model(letters, fbs)\n",
        "    preds = logits.argmax(dim=-1)\n",
        "    for j in range(5):\n",
        "        idx = j\n",
        "        print(\"Hist:\", decode_history(letters[idx], fbs[idx]))  # implement decode helper\n",
        "        print(\"Model:\", IDX2WORD[preds[idx].item()], \"Teacher:\", IDX2WORD[y[idx].item()])\n",
        "        print()\n",
        "    break\n"
      ],
      "metadata": {
        "id": "d8D1R457Pg3P"
      },
      "id": "d8D1R457Pg3P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def examples_with_green_at_position(dataset, pos):\n",
        "    # dataset is the underlying ds or train_ds/val_ds subset; assume each example __getitem__ returns letters,fbs,y\n",
        "    idxs = []\n",
        "    for i in range(len(dataset)):\n",
        "        letters, fbs, y = dataset[i]\n",
        "        fbs = np.array(fbs)\n",
        "        # fbs flattened: K*5 elements\n",
        "        if (fbs.reshape(K,5)[:, pos] == 2).any():\n",
        "            idxs.append(i)\n",
        "    return idxs\n",
        "\n",
        "# make a DataLoader for that subset:\n",
        "green_idxs = examples_with_green_at_position(val_ds.dataset if hasattr(val_ds, 'dataset') else val_ds, pos=2)\n",
        "from torch.utils.data import Subset\n",
        "green_loader = DataLoader(Subset(val_ds.dataset, green_idxs), batch_size=128)\n",
        "# then call eval_with_neuron_ablation(model, neuron_idx, green_loader)\n"
      ],
      "metadata": {
        "id": "yU66ygwWRaCL"
      },
      "id": "yU66ygwWRaCL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}